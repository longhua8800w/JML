
# æ–‡ä»¶åï¼šmlj_workflow_with_core_api.jl
# ç­–ç•¥ï¼šç”¨MLJç®¡ç†æ•°æ®ï¼Œç”¨åº“çš„æ ¸å¿ƒAPIè®­ç»ƒï¼Œå†ç”¨MLJè¯„ä¼°
using Serialization,CategoricalArrays,DataFrames,Dates
obj = deserialize("data/object.rds")

y = obj.y
X = obj.X


# =====================================================
# LightGBM ç«¯åˆ°ç«¯ AUC ä¼˜åŒ–å·¥ä½œæµ
# =====================================================
using MLJ, MLJTuning
using LightGBM, Random, Statistics, Dates, DataFrames, Serialization
using ROCAnalysis  # ç”¨äºè®¡ç®—AUC

Random.seed!(42)
println("="^70)
println("ğŸš€ LightGBM ç«¯åˆ°ç«¯ AUC ä¼˜åŒ–å·¥ä½œæµ")
println("="^70)

# 1. æ•°æ®å‡†å¤‡ä¸æ£€æŸ¥
println("\n1. ğŸ“Š æ•°æ®å‡†å¤‡ä¸æ£€æŸ¥")

# å‡è®¾ X, y å·²ç»åŠ è½½ï¼Œy æ˜¯äºŒåˆ†ç±»å‘é‡ (0/1)
println("   åŸå§‹æ•°æ®æ£€æŸ¥:")
println("   - X ç»´åº¦: $(size(X))")
println("   - y é•¿åº¦: $(length(y))")
println("   - y ç±»åˆ«åˆ†å¸ƒ: 0=$(sum(y.==0)) ($(round(mean(y.==0)*100,digits=1))%), 1=$(sum(y.==1)) ($(round(mean(y.==1)*100,digits=1))%)")

# è½¬æ¢ä¸º MLJ éœ€è¦çš„æ ¼å¼
y_cat = coerce(y, Multiclass)  # å¿…é¡»è½¬æ¢ä¸º Multiclass ç±»å‹
#coerce!(X, autotype(X, :few_to_finite))

# ä¸º AUC è®¡ç®—å‡†å¤‡çš„æ•°å€¼æ ‡ç­¾ (1 ä¸ºæ­£ç±»)
y_num = Vector{Float64}(y .== 1)

# æ•°æ®åˆ†å‰²ï¼š60%è®­ç»ƒï¼Œ20%éªŒè¯ï¼ˆè°ƒä¼˜ï¼‰ï¼Œ20%æµ‹è¯•
train_idx, temp_idx = partition(eachindex(y_cat), 0.6, shuffle=true, rng=42)
val_idx, test_idx = partition(temp_idx, 0.5, shuffle=true, rng=42)

X_train = X[train_idx, :]; y_train = y_cat[train_idx]; y_train_num = y_num[train_idx]
X_val = X[val_idx, :];   y_val = y_cat[val_idx];   y_val_num = y_num[val_idx]
X_test = X[test_idx, :]; y_test = y_cat[test_idx]; y_test_num = y_num[test_idx]

println("\n   æ•°æ®åˆ†å‰² (AUCä¼˜åŒ–):")
println("   - è®­ç»ƒé›†: $(length(train_idx)) æ ·æœ¬ (æ¨¡å‹è®­ç»ƒ)")
println("   - éªŒè¯é›†: $(length(val_idx)) æ ·æœ¬ (å‚æ•°è°ƒä¼˜)")
println("   - æµ‹è¯•é›†: $(length(test_idx)) æ ·æœ¬ (æœ€ç»ˆè¯„ä¼°)")

# 2. AUC è¯„ä¼°å‡½æ•°
println("\n2. ğŸ“ˆ å®šä¹‰ AUC è¯„ä¼°å‡½æ•°")
using MLJBase
function calculate_auc(mach, X_data, y_true)
    """
    è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®ä¸Šçš„ AUC
    """
    y_prob = MLJ.predict(mach, X_data)
    #  AUC
    res = MLJ.auc(y_prob, y_true)
    return res
end

# 3. åŠ è½½å¹¶é…ç½® LightGBM æ¨¡å‹ï¼ˆAUCä¼˜åŒ–ä¸“ç”¨ï¼‰
println("\n3. ğŸ¯ é…ç½® LightGBM (AUCä¼˜åŒ–)")

# åŠ è½½ LightGBM åˆ†ç±»å™¨
LGB = @load LGBMClassifier pkg=LightGBM

# åŸºç¡€æ¨¡å‹é…ç½® - é’ˆå¯¹ AUC ä¼˜åŒ–
base_model = LGB(
    objective="binary",
    metric=["auc"],           # ä½¿ç”¨ AUC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
    boosting="gbdt",
    verbosity=-1,           # å‡å°‘è¾“å‡º
    seed=42,
    
    # å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„å‚æ•°ï¼ˆå¦‚æœæ­£æ ·æœ¬å¾ˆå°‘ï¼‰
    scale_pos_weight=length(y_train_num)/(2*sum(y_train_num))
)

println("   åŸºç¡€æ¨¡å‹é…ç½®:")
println("   - ç›®æ ‡å‡½æ•°: binary")
println("   - è¯„ä¼°æŒ‡æ ‡: auc")
println("   - æå‡ç±»å‹: gbdt")
println("   - éšæœºç§å­: 42")
if base_model.is_unbalance
    println("   - ä¸å¹³è¡¡å¤„ç†: å¼€å¯ (æ­£æ ·æœ¬æ¯”ä¾‹=$(round(mean(y_train_num),digits=3)))")
end

# 4. å®šä¹‰è°ƒä¼˜å‚æ•°ç©ºé—´ï¼ˆAUCä¼˜åŒ–ä¸“ç”¨ï¼‰
println("\n4. âš™ï¸ å®šä¹‰è°ƒä¼˜å‚æ•°ç©ºé—´")

tuning_ranges = [
    # æ ¸å¿ƒå¤æ‚åº¦å‚æ•°
    range(base_model, :num_leaves, lower=20, upper=150, scale=:log),  # å¶å­æ•°é‡
    range(base_model, :max_depth, lower=3, upper=12),                 # æ ‘çš„æœ€å¤§æ·±åº¦
    
    # å­¦ä¹ è¿‡ç¨‹å‚æ•°
    range(base_model, :learning_rate, lower=0.01, upper=0.3, scale=:log),
    range(base_model, :num_iterations, lower=50, upper=500, scale=:log),
    
    # æ­£åˆ™åŒ–å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡AUCï¼‰
    range(base_model, :lambda_l1, lower=0.0, upper=10.0, scale=:log),  # L1æ­£åˆ™åŒ–
    range(base_model, :lambda_l2, lower=0.0, upper=10.0, scale=:log),  # L2æ­£åˆ™åŒ–
    range(base_model, :min_data_in_leaf, lower=10, upper=100, scale=:log),
    
    # éšæœºåŒ–å‚æ•°ï¼ˆæå‡æ¨¡å‹é²æ£’æ€§ï¼‰
    range(base_model, :feature_fraction, lower=0.6, upper=1.0),  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    range(base_model, :bagging_fraction, lower=0.6, upper=1.0),  # æ•°æ®é‡‡æ ·æ¯”ä¾‹
    range(base_model, :bagging_freq, lower=1, upper=10)          # baggingé¢‘ç‡
]

println("   è°ƒä¼˜å‚æ•° (9ä¸ªå…³é”®å‚æ•°):")
for (i, r) in enumerate(tuning_ranges)
    scale_info = r.scale == :log ? "[å¯¹æ•°å°ºåº¦]" : ""
    println("   $(lpad(i,2)). $(rpad(string(r.field), 20)): $(r.lower) â†’ $(r.upper) $scale_info")
end

# 5. é…ç½®é‡å¤CVè°ƒä¼˜ç­–ç•¥
println("\n5. ğŸ”„ é…ç½®é‡å¤CVè°ƒä¼˜ç­–ç•¥")

# ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯ï¼Œé…åˆéšæœºæœç´¢
cv = CV(nfolds=5, shuffle=true, rng=123)

# ä½¿ç”¨éšæœºæœç´¢ï¼ˆæ¯”è´å¶æ–¯ä¼˜åŒ–æ›´ç¨³å®šå¿«é€Ÿï¼‰
tuning = RandomSearch(rng=456)

println("   è°ƒä¼˜é…ç½®:")
println("   - äº¤å‰éªŒè¯: 5æŠ˜")
println("   - è°ƒä¼˜ç®—æ³•: éšæœºæœç´¢")
println("   - è¯„ä¼°æ¬¡æ•°: 80æ¬¡")
println("   - ä¼˜åŒ–ç›®æ ‡: æœ€å¤§åŒ–AUC")

# 6. åˆ›å»ºå¹¶æ‰§è¡Œè°ƒä¼˜
println("\n6. ğŸš€ å¼€å§‹ LightGBM AUC ä¼˜åŒ–è°ƒä¼˜...")
println("   å¼€å§‹æ—¶é—´: $(now())")

# åˆ›å»ºè°ƒä¼˜æ¨¡å‹
tuned_model = TunedModel(
    model=base_model,
    tuning=tuning,
    resampling=cv,
    ranges=tuning_ranges,
    measure=MLJ.auc,            # å…³é”®ï¼šä¼˜åŒ–AUCï¼
    n=80,                   # è¯„ä¼°80ç»„å‚æ•°
    acceleration=CPUThreads()  # ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ
)

# åœ¨éªŒè¯é›†ä¸Šæ‰§è¡Œè°ƒä¼˜ï¼ˆä¸æ˜¯è®­ç»ƒé›†ï¼ï¼‰
mach = machine(tuned_model, X_val, y_val)
MLJ.fit!(mach, verbosity=2)  # verbosity=2 æ˜¾ç¤ºè¿›åº¦æ¡

println("   ç»“æŸæ—¶é—´: $(now())")
println("   âœ… è°ƒä¼˜å®Œæˆï¼")

# 7. æå–å’Œåˆ†ææœ€ä¼˜æ¨¡å‹
println("\n7. ğŸ“Š åˆ†æè°ƒä¼˜ç»“æœ")

# è·å–è°ƒä¼˜æŠ¥å‘Š
report_df = report(mach)
best_model = fitted_params(mach).best_model
best_auc = report_df.best_history_entry.measurement[1]

println("   ğŸ† æœ€ä¼˜å‚æ•°ç»„åˆ:")
println("   - num_leaves:        $(best_model.num_leaves)")
println("   - max_depth:         $(best_model.max_depth)")
println("   - learning_rate:     $(round(best_model.learning_rate, digits=4))")
println("   - num_iterations:    $(best_model.num_iterations)")
println("   - lambda_l1:         $(round(best_model.lambda_l1, digits=4))")
println("   - lambda_l2:         $(round(best_model.lambda_l2, digits=4))")
println("   - min_data_in_leaf:  $(best_model.min_data_in_leaf)")
println("   - feature_fraction:  $(round(best_model.feature_fraction, digits=3))")
println("   - bagging_fraction:  $(round(best_model.bagging_fraction, digits=3))")
println("   - bagging_freq:      $(best_model.bagging_freq)")

println("\n   éªŒè¯é›†æ€§èƒ½:")
println("   - æœ€ä½³AUC: $(round(best_auc, digits=4))")

# æŸ¥çœ‹è°ƒä¼˜å†å²
history = report_df.history
println("   - æ€»è¯„ä¼°æ¬¡æ•°: $(length(history))")

# 8. ä½¿ç”¨æœ€ä¼˜æ¨¡å‹åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
println("\n8. ğŸ”§ è®­ç»ƒæœ€ç»ˆ LightGBM æ¨¡å‹...")

final_model = best_model
final_mach = machine(final_model, X_train, y_train)
MLJ.fit!(final_mach, verbosity=1)

println("   âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")

# 9. ç»¼åˆæ€§èƒ½è¯„ä¼°
println("\n9. ğŸ§ª ç»¼åˆæ€§èƒ½è¯„ä¼°")

# 9.1 è®­ç»ƒé›† AUC

train_auc = calculate_auc(final_mach, X_train, y_train)
println("   è®­ç»ƒé›† AUC: $(round(train_auc, digits=4))")

# 9.2 éªŒè¯é›† AUCï¼ˆè°ƒä¼˜æ—¶å·²çœ‹è¿‡ï¼Œè¿™é‡Œå†ç¡®è®¤ï¼‰
val_auc = calculate_auc(final_mach, X_val, y_val)
println("   éªŒè¯é›† AUC: $(round(val_auc, digits=4))")

# 9.3 æµ‹è¯•é›† AUCï¼ˆæœ€é‡è¦ï¼ï¼‰
test_auc = calculate_auc(final_mach, X_test, y_test)
println("   æµ‹è¯•é›† AUC: $(round(test_auc, digits=4))")

# 9.4 å‡†ç¡®ç‡ç­‰å…¶ä»–æŒ‡æ ‡ï¼ˆä½œä¸ºå‚è€ƒï¼‰
y_pred_test = predict_mode(final_mach, X_test)
accuracy_test = mean(y_pred_test .== y_test)
precision_test = mean(y_pred_test[y_test.=="1"] .== "1")
recall_test = mean(y_test[y_pred_test.=="1"] .== "1")

println("\n   æµ‹è¯•é›†å…¶ä»–æŒ‡æ ‡ (å‚è€ƒ):")
println("   - å‡†ç¡®ç‡:    $(round(accuracy_test*100, digits=2))%")
println("   - ç²¾ç¡®ç‡:    $(round(precision_test*100, digits=2))%")
println("   - å¬å›ç‡:    $(round(recall_test*100, digits=2))%")

# 10. ç‰¹å¾é‡è¦æ€§åˆ†æ
println("\n10. ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ")



using LightGBM  # ç¡®ä¿åŠ è½½

# è·å– fitted_params
fp = fitted_params(final_mach)

# ä» Tuple ä¸­æå– Estimatorï¼ˆLGBMClassificationï¼Œå³ Tuple[1]ï¼‰
lgbm_estimator = fp.fitresult[1]  # Tuple çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ LGBMClassification (LGBMEstimator å­ç±»å‹)

# è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆ"gain" ç±»å‹ï¼›å¯æŒ‡å®šè¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤æ‰€æœ‰ï¼‰
importances_gain = LightGBM.gain_importance(lgbm_estimator)  # é»˜è®¤æ‰€æœ‰è¿­ä»£
# æˆ–æŒ‡å®šè¿­ä»£ï¼šimportances_gain = LightGBM.gain_importance(lgbm_estimator, 100)  # åŸºäºå‰ 100 è¿­ä»£

# è·å–ç‰¹å¾åï¼ˆä» X æ•°æ®ä¸­æå–ï¼›å‡è®¾ X æ˜¯ tableï¼‰
feature_names = schema(X).names  # e.g., [:feat1, :feat2, ...]

# è¾“å‡ºæ’åºåçš„é‡è¦æ€§
sorted_indices = sortperm(importances_gain, rev=true)  # é™åº
println("ç‰¹å¾é‡è¦æ€§ (åŸºäº Gain):")
for i in sorted_indices
    println("  ç‰¹å¾ $(feature_names[i]): $(importances_gain[i])")
end

# å¦‚æœæƒ³ç”¨ "split" ç±»å‹ï¼ˆåŸºäºåˆ†è£‚æ¬¡æ•°ï¼‰ï¼š
importances_split = LightGBM.split_importance(lgbm_estimator)
# ç„¶ååŒæ ·æ’åºè¾“å‡º

# å¯è§†åŒ–ï¼ˆå¯é€‰ï¼Œç”¨ Plots.jlï¼‰
using Plots

feature_labels = String.(collect(feature_names[sorted_indices])) 
bar(feature_labels, importances_gain[sorted_indices], 
    title="LightGBM Feature Importances (Gain)", 
    xlabel="Features", ylabel="Importance", orientation=:h)

# 11. æ¨¡å‹ä¿å­˜
println("\n11. ğŸ’¾ ä¿å­˜æ¨¡å‹ä¸ç»“æœ")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
MLJ.save("mdls/lightgbm_auc_optimized_final.jlso", final_mach)

# ä¿å­˜è°ƒä¼˜å†å²ï¼ˆåŒ…å«æ‰€æœ‰å°è¯•çš„å‚æ•°ç»„åˆï¼‰
tuning_history = Dict(
    :best_model => best_model,
    :best_auc => best_auc,
    :test_auc => test_auc,
    :feature_importance => fi,
    :all_history => [(h.model, h.measurement[1]) for h in history]
)


# =====================================================
# å…³é”®æŒ‡æ ‡æ€»ç»“
# =====================================================
println("\nğŸ“Š å…³é”®æŒ‡æ ‡æ€»ç»“:")
println("   â€¢ éªŒè¯é›†æœ€ä½³AUC: $(round(best_auc, digits=4))")
println("   â€¢ æµ‹è¯•é›†AUC:     $(round(test_auc, digits=4))")
println("   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡:  $(round(accuracy_test*100, digits=2))%")
println("   â€¢ ç‰¹å¾æ•°é‡:      $(ncol(X))")
println("   â€¢ æœ€ä¼˜è¿­ä»£æ¬¡æ•°:  $(best_model.num_iterations)")
println("   â€¢ å­¦ä¹ ç‡:        $(round(best_model.learning_rate, digits=4))")