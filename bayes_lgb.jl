
# æ–‡ä»¶åï¼šmlj_workflow_with_core_api.jl
# ç­–ç•¥ï¼šç”¨MLJç®¡ç†æ•°æ®ï¼Œç”¨åº“çš„æ ¸å¿ƒAPIè®­ç»ƒï¼Œå†ç”¨MLJè¯„ä¼°
using Serialization,CategoricalArrays,DataFrames,Dates



obj = deserialize("data/object.rds")

y = obj.y
X = obj.X


# =====================================================
# LightGBM ç«¯åˆ°ç«¯ AUC ä¼˜åŒ–å·¥ä½œæµ
# =====================================================
using MLJ, MLJTuning
using LightGBM, Random, Statistics
using ROCAnalysis  # ç”¨äºŽè®¡ç®—AUC

Random.seed!(42)

# è½¬æ¢ä¸º MLJ éœ€è¦çš„æ ¼å¼
y_cat = coerce(y, Multiclass)  # å¿…é¡»è½¬æ¢ä¸º Multiclass ç±»åž‹
#coerce!(X, autotype(X, :few_to_finite))

PCT_TRAIN_DATA = 0.75
# æ•°æ®åˆ†å‰²ï¼šè®­ç»ƒï¼Œæµ‹è¯•
train_idx, test_idx = partition(eachindex(y_cat), PCT_TRAIN_DATA, shuffle=true, rng=42)

X_train = X[train_idx, :]; y_train = y_cat[train_idx]
X_test = X[test_idx, :]; y_test = y_cat[test_idx]


# 2. AUC è¯„ä¼°å‡½æ•°

using MLJBase
function calculate_auc(mach, X_data, y_true)
    y_prob = MLJ.predict(mach, X_data)
    res = MLJ.auc(y_prob, y_true)
    return res
end


# åŠ è½½ LightGBM åˆ†ç±»å™¨
LGB = @load LGBMClassifier pkg=LightGBM

# åŸºç¡€æ¨¡åž‹é…ç½® - é’ˆå¯¹ AUC ä¼˜åŒ–
base_model = LGB(
    objective="binary",
    metric=["auc"],           # ä½¿ç”¨ AUC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
    boosting="gbdt",
    verbosity=-1,           # å‡å°‘è¾“å‡º
    seed=42,
    is_unbalance=true       # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
)



# 5. é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–
println("\n5. ðŸŽ¯ é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–")

using TreeParzen

# å°† MLJ ranges è½¬æ¢ä¸º TreeParzen çš„å…ˆéªŒåˆ†å¸ƒ
space = Dict{Symbol, Any}(
    # è¿­ä»£æ¬¡æ•°ï¼ˆå¯¹åº”num_roundï¼‰
    :num_iterations => TreeParzen.HP.QuantUniform(:num_iterations, 50.0, 500.0, 1.0),
    
    # å­¦ä¹ çŽ‡ï¼ˆå¯¹åº”etaï¼‰
    :learning_rate => TreeParzen.HP.LogUniform(:learning_rate, log(0.01), log(0.3)),
    
    # å¶å­æ•°é‡ï¼ˆLightGBMç‰¹æœ‰ï¼Œå¯¹åº”max_depthä½†ä¸åŒï¼‰
    :num_leaves => TreeParzen.HP.QuantUniform(:num_leaves, 20.0, 150.0, 1.0),
    
    # æœ€å¤§æ·±åº¦
    :max_depth => TreeParzen.HP.QuantUniform(:max_depth, 3.0, 12.0, 1.0),
    
    # L1æ­£åˆ™åŒ–ï¼ˆå¯¹åº”alphaï¼‰
    :lambda_l1 => TreeParzen.HP.LogUniform(:lambda_l1, log(0.001), log(10.0)),
    
    # L2æ­£åˆ™åŒ–ï¼ˆå¯¹åº”lambdaï¼‰
    :lambda_l2 => TreeParzen.HP.LogUniform(:lambda_l2, log(0.001), log(10.0)),
    
    # æœ€å°å¶å­æ ·æœ¬æ•°ï¼ˆå¯¹åº”min_child_weightä½†ä¸åŒï¼‰
    :min_data_in_leaf => TreeParzen.HP.QuantUniform(:min_data_in_leaf, 10.0, 100.0, 1.0),
    
    # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    :feature_fraction => TreeParzen.HP.Uniform(:feature_fraction, 0.6, 1.0),
    
    # æ•°æ®é‡‡æ ·æ¯”ä¾‹
    :bagging_fraction => TreeParzen.HP.Uniform(:bagging_fraction, 0.6, 1.0),
    
    # baggingé¢‘çŽ‡
    :bagging_freq => TreeParzen.HP.QuantUniform(:bagging_freq, 1.0, 10.0, 1.0)
)

println("å·²åˆ›å»º $(length(space)) ä¸ªå‚æ•°çš„å…ˆéªŒåˆ†å¸ƒ")

# æŸ¥çœ‹åˆ›å»ºçš„å…ˆéªŒ
println("\nå…ˆéªŒåˆ†å¸ƒé…ç½®:")
for (key, prior) in space
    println("  $key: $prior")
end


# åˆ›å»º TreeParzen è°ƒä¼˜å™¨
# åˆ›å»º TreeParzen è°ƒä¼˜å™¨
# 5. é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–
println("\n5. ðŸŽ¯ é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–")


NUM_CV_FOLDS = 5
NUM_TP_ITER_SMALL = 30
NUM_TP_ITER_LARGE = length(space)*50

tuning = MLJTuning.TunedModel(
    model=base_model,
    range=space,
    tuning=MLJTreeParzenTuning(max_simultaneous_draws=4),
    n=NUM_TP_ITER_SMALL,
    resampling=MLJ.CV(nfolds=NUM_CV_FOLDS),
    measure=MLJ.auc,
)



mach = MLJ.machine(tuning, X_train, y_train)

println("å¼€å§‹æ—¶é—´: $(now())")
MLJ.fit!(mach, verbosity=2)
println("ç»“æŸæ—¶é—´: $(now())")



best_model = MLJ.fitted_params(mach).best_model

suggestion = Dict(key => getproperty(best_model, key) for key in keys(space))

search = MLJTreeParzenSpace(space, suggestion)

tuning2 = MLJTuning.TunedModel(
    model=base_model,
    range=search,
    tuning=MLJTreeParzenTuning(max_simultaneous_draws=2),
    n=NUM_TP_ITER_SMALL,
    resampling=MLJ.CV(nfolds=NUM_CV_FOLDS),
    measure=MLJ.auc,
)


mach2 = MLJ.machine(tuning2, X_train, y_train)

println("å¼€å§‹æ—¶é—´: $(now())")
MLJ.fit!(mach2, verbosity=2)
println("ç»“æŸæ—¶é—´: $(now())")




best_model2 = MLJ.fitted_params(mach2).best_model

suggestion2 = Dict(key => getproperty(best_model2, key) for key in keys(space))

search2 = MLJTreeParzenSpace(space, suggestion2)

using ComputationalResources
tuning3 = MLJTuning.TunedModel(
    model=base_model,
    range=search2,
    tuning=MLJTreeParzenTuning(max_simultaneous_draws=2),
    n=NUM_TP_ITER_SMALL,
    resampling=MLJ.CV(nfolds=NUM_CV_FOLDS),
    measure=MLJ.auc,
    acceleration=ComputationalResources.CPUProcesses(),
)


mach3 = MLJ.machine(tuning3, X_train, y_train)

println("å¼€å§‹æ—¶é—´: $(now())")
MLJ.fit!(mach3, verbosity=2)
println("ç»“æŸæ—¶é—´: $(now())")

report(mach3)

best_model3 = fitted_params(mach3).best_model

MLJ.save("mdls/best_model3.jls", best_model3)




# 2. ä½¿ç”¨æœ€ä¼˜æ¨¡åž‹ï¼Œåœ¨å®Œæ•´çš„è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒä¸€ä¸ªä¸“é—¨çš„â€œæŽ¨ç†æœºå™¨â€
inference_mach = machine(best_model3, X_train, y_train)
MLJ.fit!(inference_mach)

using JLSO

# This machine can now be serialized
smach = serializable(mach3)
JLSO.save("mdls/machine.jlso", :machine => smach)

# Deserialize and restore learned parameters to useable form:
loaded_mach = JLSO.load("mdls/machine.jlso")[:machine]
restore!(loaded_mach)

MLJ.predict(loaded_mach, X_test)
MLJ.predict(mach3, X_test)
