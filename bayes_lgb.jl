
# æ–‡ä»¶åï¼šmlj_workflow_with_core_api.jl
# ç­–ç•¥ï¼šç”¨MLJç®¡ç†æ•°æ®ï¼Œç”¨åº“çš„æ ¸å¿ƒAPIè®­ç»ƒï¼Œå†ç”¨MLJè¯„ä¼°
using Serialization,CategoricalArrays,DataFrames,Dates
obj = deserialize("data/object.rds")

y = obj.y
X = obj.X


# =====================================================
# LightGBM ç«¯åˆ°ç«¯ AUC ä¼˜åŒ–å·¥ä½œæµ
# =====================================================
using MLJ, MLJTuning
using LightGBM, Random, Statistics
using ROCAnalysis  # ç”¨äºè®¡ç®—AUC

Random.seed!(42)

# è½¬æ¢ä¸º MLJ éœ€è¦çš„æ ¼å¼
y_cat = coerce(y, Multiclass)  # å¿…é¡»è½¬æ¢ä¸º Multiclass ç±»å‹
#coerce!(X, autotype(X, :few_to_finite))

# æ•°æ®åˆ†å‰²ï¼š60%è®­ç»ƒï¼Œ20%éªŒè¯ï¼ˆè°ƒä¼˜ï¼‰ï¼Œ20%æµ‹è¯•
train_idx, temp_idx = partition(eachindex(y_cat), 0.6, shuffle=true, rng=42)
val_idx, test_idx = partition(temp_idx, 0.5, shuffle=true, rng=42)

X_train = X[train_idx, :]; y_train = y_cat[train_idx]
X_val = X[val_idx, :];   y_val = y_cat[val_idx]
X_test = X[test_idx, :]; y_test = y_cat[test_idx]


# 2. AUC è¯„ä¼°å‡½æ•°

using MLJBase
function calculate_auc(mach, X_data, y_true)
    y_prob = MLJ.predict(mach, X_data)
    res = MLJ.auc(y_prob, y_true)
    return res
end


# åŠ è½½ LightGBM åˆ†ç±»å™¨
LGB = @load LGBMClassifier pkg=LightGBM

# åŸºç¡€æ¨¡å‹é…ç½® - é’ˆå¯¹ AUC ä¼˜åŒ–
base_model = LGB(
    objective="binary",
    metric=["auc"],           # ä½¿ç”¨ AUC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
    boosting="gbdt",
    verbosity=-1,           # å‡å°‘è¾“å‡º
    seed=42,
    is_unbalance=true       # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
)



# 4. å®šä¹‰è°ƒä¼˜å‚æ•°ç©ºé—´ï¼ˆAUCä¼˜åŒ–ä¸“ç”¨ï¼‰

tuning_ranges = [
    # æ ¸å¿ƒå¤æ‚åº¦å‚æ•°
    range(base_model, :num_leaves, lower=20, upper=150, scale=:log),  # å¶å­æ•°é‡
    range(base_model, :max_depth, lower=3, upper=12),                 # æ ‘çš„æœ€å¤§æ·±åº¦
    
    # å­¦ä¹ è¿‡ç¨‹å‚æ•°
    range(base_model, :learning_rate, lower=0.01, upper=0.3, scale=:log),
    range(base_model, :num_iterations, lower=50, upper=500, scale=:log),
    
    # æ­£åˆ™åŒ–å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡AUCï¼‰
    range(base_model, :lambda_l1, lower=0.0, upper=10.0, scale=:log),  # L1æ­£åˆ™åŒ–
    range(base_model, :lambda_l2, lower=0.0, upper=10.0, scale=:log),  # L2æ­£åˆ™åŒ–
    range(base_model, :min_data_in_leaf, lower=10, upper=100, scale=:log),
    
    # éšæœºåŒ–å‚æ•°ï¼ˆæå‡æ¨¡å‹é²æ£’æ€§ï¼‰
    range(base_model, :feature_fraction, lower=0.6, upper=1.0),  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    range(base_model, :bagging_fraction, lower=0.6, upper=1.0),  # æ•°æ®é‡‡æ ·æ¯”ä¾‹
    range(base_model, :bagging_freq, lower=1, upper=10)          # baggingé¢‘ç‡
]

println("   è°ƒä¼˜å‚æ•° (9ä¸ªå…³é”®å‚æ•°):")
for (i, r) in enumerate(tuning_ranges)
    scale_info = r.scale == :log ? "[å¯¹æ•°å°ºåº¦]" : ""
    println("   $(lpad(i,2)). $(rpad(string(r.field), 20)): $(r.lower) â†’ $(r.upper) $scale_info")
end






# 5. é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–
println("\n5. ğŸ¯ é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–")

using TreeParzen

# å°† MLJ ranges è½¬æ¢ä¸º TreeParzen çš„å…ˆéªŒåˆ†å¸ƒ
priors = Dict{Symbol, Any}(
    # è¿­ä»£æ¬¡æ•°ï¼ˆå¯¹åº”num_roundï¼‰
    :num_iterations => TreeParzen.HP.QuantUniform(:num_iterations, 50.0, 500.0, 1.0),
    
    # å­¦ä¹ ç‡ï¼ˆå¯¹åº”etaï¼‰
    :learning_rate => TreeParzen.HP.LogUniform(:learning_rate, log(0.01), log(0.3)),
    
    # å¶å­æ•°é‡ï¼ˆLightGBMç‰¹æœ‰ï¼Œå¯¹åº”max_depthä½†ä¸åŒï¼‰
    :num_leaves => TreeParzen.HP.QuantUniform(:num_leaves, 20.0, 150.0, 1.0),
    
    # æœ€å¤§æ·±åº¦
    :max_depth => TreeParzen.HP.QuantUniform(:max_depth, 3.0, 12.0, 1.0),
    
    # L1æ­£åˆ™åŒ–ï¼ˆå¯¹åº”alphaï¼‰
    :lambda_l1 => TreeParzen.HP.LogUniform(:lambda_l1, log(0.001), log(10.0)),
    
    # L2æ­£åˆ™åŒ–ï¼ˆå¯¹åº”lambdaï¼‰
    :lambda_l2 => TreeParzen.HP.LogUniform(:lambda_l2, log(0.001), log(10.0)),
    
    # æœ€å°å¶å­æ ·æœ¬æ•°ï¼ˆå¯¹åº”min_child_weightä½†ä¸åŒï¼‰
    :min_data_in_leaf => TreeParzen.HP.QuantUniform(:min_data_in_leaf, 10.0, 100.0, 1.0),
    
    # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    :feature_fraction => TreeParzen.HP.Uniform(:feature_fraction, 0.6, 1.0),
    
    # æ•°æ®é‡‡æ ·æ¯”ä¾‹
    :bagging_fraction => TreeParzen.HP.Uniform(:bagging_fraction, 0.6, 1.0),
    
    # baggingé¢‘ç‡
    :bagging_freq => TreeParzen.HP.QuantUniform(:bagging_freq, 1.0, 10.0, 1.0)
)

println("å·²åˆ›å»º $(length(priors)) ä¸ªå‚æ•°çš„å…ˆéªŒåˆ†å¸ƒ")

# æŸ¥çœ‹åˆ›å»ºçš„å…ˆéªŒ
println("\nå…ˆéªŒåˆ†å¸ƒé…ç½®:")
for (key, prior) in priors
    println("  $key: $prior")
end


# åˆ›å»º TreeParzen è°ƒä¼˜å™¨
# åˆ›å»º TreeParzen è°ƒä¼˜å™¨
# 5. é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–
println("\n5. ğŸ¯ é…ç½® TreeParzen è´å¶æ–¯ä¼˜åŒ–")

using TreeParzen

NUM_CV_FOLDS = 4
PCT_TRAIN_DATA = 0.75
NUM_TP_ITER_SMALL = 25
NUM_TP_ITER_LARGE = 250

tuning = MLJTuning.TunedModel(
    model=base_model,
    range=priors,
    tuning=MLJTreeParzenTuning(),
    n=NUM_TP_ITER_SMALL,
    resampling=MLJ.CV(nfolds=NUM_CV_FOLDS),
    measure=MLJ.auc,
)



mach = MLJ.machine(tuning, X_train, y_train)

println("å¼€å§‹æ—¶é—´: $(now())")
MLJ.fit!(mach, verbosity=2)
println("ç»“æŸæ—¶é—´: $(now())")



best_model = MLJ.fitted_params(mach).best_model

suggestion = Dict(key => getproperty(best_model, key) for key in keys(priors))

search = MLJTreeParzenSpace(priors, suggestion)

tuning2 = MLJTuning.TunedModel(
    model=base_model,
    range=search,
    tuning=MLJTreeParzenTuning(;random_trials=3),
    n=NUM_TP_ITER_SMALL,
    resampling=MLJ.CV(nfolds=NUM_CV_FOLDS),
    measure=MLJ.auc,
)


mach2 = MLJ.machine(tuning2, X_train, y_train)

println("å¼€å§‹æ—¶é—´: $(now())")
MLJ.fit!(mach2, verbosity=2)
println("ç»“æŸæ—¶é—´: $(now())")





tuning21 = MLJTuning.TunedModel(
    model=base_model,
    range=search,
    tuning=MLJTreeParzenTuning(;random_trials=3, max_simultaneous_draws=2, linear_forgetting=50),
    n=NUM_TP_ITER_SMALL,
    resampling=MLJ.CV(nfolds=NUM_CV_FOLDS),
    measure=MLJ.auc,
)




mach21 = MLJ.machine(tuning21, X_train, y_train)

println("å¼€å§‹æ—¶é—´: $(now())")
MLJ.fit!(mach, verbosity=2)
println("ç»“æŸæ—¶é—´: $(now())")
